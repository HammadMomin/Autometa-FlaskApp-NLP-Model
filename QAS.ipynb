{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7a918",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ans_file=\"\"\n",
    "text=\"\"\n",
    "question=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57103516",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import re\n",
    "import time\n",
    "import papermill as pm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33d3d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Model and Encoder\n",
    "model=torch.load(\"MODEL_APR_23\")\n",
    "model.eval()\n",
    "tokenizer = torch.load(\"tokenizer_encoder.unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c02754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer_A(question, text):\n",
    "    \n",
    "    #tokenize question and text in ids as a pair\n",
    "    input_ids = tokenizer.encode(question, text)\n",
    "    \n",
    "    #string version of tokenized ids\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    #segment IDs\n",
    "    #first occurence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    #number of tokens in segment A - question\n",
    "    num_seg_a = sep_idx+1\n",
    "\n",
    "    #number of tokens in segment B - text\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    #list of 0s and 1s\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    \n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    #model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "    #reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "    answer = 'unable to find answer'\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    return torch.max(output.start_logits), answer.capitalize() #score ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b2af179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len):\n",
    "    # split the text into sentences\n",
    "    sentences = re.findall('[^.!?]+[.!?]', text)\n",
    "    \n",
    "    # initialize variables\n",
    "    current_len = 0\n",
    "    current_chunk = []\n",
    "    chunks = []\n",
    "    \n",
    "    # iterate over each sentence\n",
    "    for sentence in sentences:\n",
    "        # add the sentence to the current chunk\n",
    "        current_chunk.append(sentence)\n",
    "        \n",
    "        # update the length of the current chunk\n",
    "        current_len += len(sentence)\n",
    "        \n",
    "        # if the length of the current chunk is greater than or equal to the max length\n",
    "        if current_len >= max_len:\n",
    "            # add the current chunk to the list of chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            \n",
    "            # reset the current chunk and length\n",
    "            current_chunk = []\n",
    "            current_len = 0\n",
    "    \n",
    "    # add any remaining sentences as a final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2cbd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_better_ans(question, text):\n",
    "  chunk_of_text=chunk_text(text,450)\n",
    "  scores = {}\n",
    "  for i in chunk_of_text:\n",
    "    scores.update({question_answer_A(question, i)[0] : question_answer_A(question, i)[1] })\n",
    "  v = list(scores.values())\n",
    "  k = list(scores.keys())\n",
    "  return (v[k.index(max(k))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5836bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)\n",
    "print(question)\n",
    "x = get_better_ans(question, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38069785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aircraft\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "f = open(ans_file, \"w\")\n",
    "f.write(x)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795da226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
